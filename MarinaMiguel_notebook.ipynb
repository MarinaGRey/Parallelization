{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a055138b-2e6a-41c8-97a8-77806a8b68b0",
   "metadata": {},
   "source": [
    "# First Practical Work (Massive Computing)\n",
    "#### Marina Gómez Rey (100472836) and Miguel Fernández Lara (100473125). Group 96\n",
    "##### 20/10/2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b849b0-a75d-4020-90a7-da5c8621e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MarinaMiguelFunctions as my\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.sharedctypes import Value, Array, RawArray\n",
    "from multiprocessing import Process, Lock\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70006580-dabb-436a-9fb8-2b699a163a07",
   "metadata": {},
   "source": [
    "### Generating invertible matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f3f819-0cb6-4868-8914-4ba5ec0af7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_invertible_matrix(size):\n",
    "    while True:\n",
    "        \n",
    "        # We are using random.uniform but any other ranges of numbers could be used\n",
    "        matrix = [[random.uniform(-1, 1) for _ in range(size)] for _ in range(size)]\n",
    "        \n",
    "        if np.linalg.det(matrix) != 0: # Allowed used for numpy for checking the determinant of the matrix\n",
    "            return matrix\n",
    "\n",
    "matrix_size = 100 # Change the number here to test multiple sizes\n",
    "\n",
    "matrix = generate_invertible_matrix(matrix_size) # Generate a random invertible matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39878293-099b-4db3-a8b8-19f0f0f4e0c8",
   "metadata": {},
   "source": [
    "### Initializing the shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d7bb4f-0323-4225-972f-5f4abfd1a0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "data_buffer_size=len(matrix)**2 # The length of the data buffer must be n*n\n",
    "\n",
    "print(data_buffer_size)  # Check the size of the data buffer, which will be stored into the shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15acc774-c1b3-40d7-90d9-8f16d265c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Parametrization of the number of cores\n",
    "\n",
    "IS_HT=True #True if your computer have the HyperThreading feature activated, False if not.\n",
    "NUMREPORTEDCORES=mp.cpu_count() \n",
    "# The number of real computational cores\n",
    "if IS_HT:\n",
    "    NUMCORES=(int) (NUMREPORTEDCORES/2)\n",
    "else:\n",
    "    NUMCORES=NUMREPORTEDCORES\n",
    "\n",
    "print(NUMCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45db3f29-7f42-4bf7-87ab-a2f8611fdb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "shared_space= Array(ctypes.c_longlong, data_buffer_size) # Initialize the shared memory space with longlong c type (longest one we found)\n",
    "\n",
    "shared_data=my.np.frombuffer(shared_space.get_obj(),dtype=np.float64) # Type float for large numbers\n",
    "\n",
    "result_matrix = shared_data.reshape(len(matrix), len(matrix))  # In this matrix we will need to get the minors\n",
    "\n",
    "print(result_matrix.shape) # Check the dimensions of the result matrix. It must be nxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8559ba86-e9c6-48d4-871d-bf02639b5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 84.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5439920374013284e+52"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "my.determinant(matrix)  # Here we check how much the determinant function takes (LU factorization O(n^3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8d122-c0b6-4295-9dae-c7419cd988d2",
   "metadata": {},
   "source": [
    "### Parallelization per ROW using method of minors with LU determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa600ae-9e55-4423-8ff0-e5447e142a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = len(matrix) # Retrieve the length of the matrix: n\n",
    "\n",
    "row_indices = list(range(n)) # List of row indices to process\n",
    "\n",
    "# Parallelize by rows\n",
    "# The init arguments must be: shared space (to store the results in the shared memory), matrix shape (to reshape shared memory array) and \n",
    "# the current matrix (for it to become a global variable)\n",
    "# We are using the multiprocessing library with the Pool method and map function.\n",
    "with mp.Pool(processes=NUMCORES, initializer=my.init_sharedarray, initargs=[shared_space, (n, n), matrix]) as p:\n",
    "    p.map(my.calculate_minor_for_row, row_indices)\n",
    "\n",
    "# Now we have stored in the shared memory the whole matrix of minors\n",
    "\n",
    "det_matrix = my.determinant(matrix) # Calculate the determinant of the whole matrix\n",
    "\n",
    "invert_det = 1/det_matrix # Invert the value of the determinant\n",
    "\n",
    "matrix_c = my.cofactor_matrix(result_matrix) # Get the cofactor matrix (multiply by 1 or -1)\n",
    "\n",
    "matrix_t = my.transpose_matrix(matrix_c) # Transpose the matrix\n",
    "\n",
    "inverse = my.multiply_number_by_matrix(invert_det,matrix_t) # Multiply the transposed matrix by the inverse of the determinant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1422e-264e-42d2-b639-0d21ae3b3b9f",
   "metadata": {},
   "source": [
    "### Checking the solution (per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84c02ddc-c73c-4fca-815e-4cd8e9948470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "result = np.matmul(inverse, matrix) # We should get the identity\n",
    "\n",
    "tolerance = 1e-7 # Some tolerance because they will not be exact 0's. \n",
    "\n",
    "identity = np.eye(n) # Numpy identity\n",
    "    \n",
    "print(np.allclose(result, identity, atol=tolerance)) # Print TRUE if the computation of the inverse is correct\n",
    "\n",
    "# The lower the tolerance, the more probability of getting a False in the allclose method. It does not mean that the result is incorrect,\n",
    "# it means that the significant figures taken were different and the round up was performed differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda9fcc-7f0d-483d-b9d6-60878db2e704",
   "metadata": {},
   "source": [
    "# Conclusion First Practical Work \n",
    "\n",
    "#### Introduction\n",
    "\n",
    "The main goal of this first practical work is to understand how processes can be parallelized using the CPU and shared memory in our computers in order to make large complexity operations easily. In this case, we had to implement matrix inversion without using the numpy library. In order to perform the inversion, we chose to follow the method detailed in the practical work statement, since the computation of the minors is a highly parallelizable process and we wanted to reduce the time consumption in this process. \n",
    "\n",
    "The first step was to generate the invertible matrix, which could be done using numpy to check if the determinant is equal to 0. Then, we had to find a method to compute the matrix of minors. This is a high complexity process because for every cell given in the matrix there is a minor, which is computed finding a determinant of a matrix (n-1)x(n-1), which is the one resulting after removing the column and row of the target cell. Finally, we just had to perform the rest of sequential straightforward operations to find the inverse, which included finding the cofactor matrix (multiplying by 1 or -1), transposing this resulting matrix, and multiplying it by the inverse of the determinant of the initial matrix.\n",
    "\n",
    "\n",
    "#### The parallelization process\n",
    "\n",
    "Before starting the parallel processing, the shared memory had to be initilized. The shared space is an array of nxn positions, in which we will store \"longlong\" ctypes. It is the largest data type we could found in order to be able to store as many significant figures of the large determinant values computed from extremely big matrices. We have succesfully tested that having this ctype does not produce any overhead and therefore we decided to keep it. Besides, the shared data was initialized to store float64 numbers. This meant that the result of the determinats would be represented in floating point arithmetic. Finally, a result matrix was created by reshaping the shared space array, giving a more visual representation of the result while keeping the shared memory array format.\n",
    "\n",
    "The multiprocessing library was used to implement the parallelization by using the \"Pool\" objects. We have decided to parallelize by rows as it has been the approach that led to the best results.\n",
    "\n",
    "The \"Pool\" object created used the \"map\" method to parallelize, taking as arguments the indices of each row (a list with numbers from 0 to n-1) and as init arguments we passed the matrix (to create a global variable), the shared space and the matrix size.\n",
    "\n",
    "#### Retreiving the minors\n",
    "\n",
    "For every row input in the \"map\" method, we then computed sequentially the minors of each cell. Therefore, we needed to first find the most optimal method to retrieve the minor. This was achieved using vectorized operations instead of two nested for loops. The matrix resulting is the one that takes the indices of all rows unless the current one, and the same operation for the columns. This process is insignificant in terms of time consumption.\n",
    "\n",
    "\n",
    "#### Finding the determinant\n",
    "\n",
    "Finding the correct way to perform the determinant was a hustle, as we could not use numpy nor recursion. The lowest complexity algorithms we could think of had a complexity of O(n^3), which means three nested \"for loops\". After days of research, we could not find a more optimal way to do this. Therefore, we had to keep one of the cubic complexity method, which, by testing, we found the fastest was to decompose the matrix into LU factorization using gaussian elimination, which then provided the determinant of the matrix easily. However, this method is not highly scalable, since it takes a large amount of time for matrices over 200x200.\n",
    "\n",
    "\n",
    "#### Storing in the shared memory\n",
    "\n",
    "After the complete row of determinants is computed, it needs to be stored into the shared memory. To do this, we have implemented locks to provide a secure upload and retreival of data, avoiding overlapping of processes. After the lock is initialized, the complete row of determinants is stored inside the result matrix (reshaped shared space array).\n",
    "\n",
    "#### Problems\n",
    "\n",
    "The main problem arises with the overall wall time. Whilst the CPU time of the code is very low, the wall time is exponentially higher. Therefore, the conclusion we reached is that the overhead of the process is very high, which means that it is taking a huge amount of time to pass results to the shared memory and back, along with initializing the original matrix for each minor. However, we need to also take into consideration that the wall time also includes the queue from processes running inside our computer, which means that actually the timing to perform the matrix inversion is very low.\n",
    "Regarding this problem, we decided to reduce by overhead by computing the parallelization by blocks instead of by rows, but that approach did not provide better results. Also, it is important to note that we divided by rows instead of by cells (fine granularity) to reduce this problem.\n",
    "\n",
    "#### Other tried approaches\n",
    "\n",
    "- LU decomposition: this technique consists on dividing the matrix into the multiplication of two. A lower triagular (L) and an upper triangular(U). Then, in order to compute the inverse the decomposition must be done and solving two triangular systems for each column of the identity matrix you obtain the inverse. We tried parallelizing this process by blocks or by columns. However, the results did not improve the previously explained method, so we remained with it.\n",
    "\n",
    "- Gaussian elimination: this process involves converting the matrix into an upper triangular one through partial pivoting, scaling pivot rows, and eliminating entries below the pivot. Following this, the back substitution process is applied, which involves starting from the last row and eliminating values in the current column of all preceding rows while updating an identity matrix to construct the inverse. The process itself is extremely fast, making the inverse of a 100x100 matrix in less than two seconds. However, when parallelizing, the most expensive process is the eliminate the current column in other rows. We tried parallelizing this process keeping the rest sequential. However, that process is inside a loop and, overall it did not work as expected so we decided again to return to the original idea.\n",
    "\n",
    "#### Timing Results\n",
    "\n",
    "Using a MacBook Air 2020, with an Apple Silicon M1 processor (8 cores), the average times obtained were as follows:\n",
    "\n",
    "- 10x10 matrix: 60 ms\n",
    "- 50x50 matrix: 1.3 s\n",
    "- 100x100 matrix: 57 s\n",
    "\n",
    "We expect better performance on a machine with more cores and/or higher computational capacity.\n",
    "\n",
    "#### Overall\n",
    "\n",
    "As a final conclusion, we are satisfied with the performance of our implementation and we have succesfully learnt how to implement CPU parallelization in a fairly optimal way. We have also seen the importance of deciding which processes require to be parallelized and which ones can be sequential, because if everything is computed in parallel, issues with the locks can provide worse results than expected. The time results were not as expected in the original requirements, so we are very looking forward to seeing the solution for a further insight on our mistakes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
